# How-to-Deploy-Rook-Ceph-on-Kubernetes---The-Ultimate-Step-by-Step-Guide-for-Cloud-Native-Storage
How to Deploy Rook Ceph on Kubernetesâ€Š-â€ŠThe Ultimate Step-by-Step Guide for Cloud-Native Storage
# ğŸš€ How to Deploy Rook Ceph on Kubernetes â€” The Ultimate Step-by-Step Guide for Cloud-Native Storage

Kubernetes makes deploying applications easy.  
But what about **persistent storage** â€” the data that actually matters?  
Thatâ€™s where things get tricky.  

If youâ€™ve ever struggled to make databases or stateful workloads reliable on Kubernetes, youâ€™ve probably heard of **Ceph** â€” a powerful, distributed storage system that can handle block, file, and object storage at scale.  

But deploying Ceph manually is complex.  
Thatâ€™s where **Rook** comes in â€” a Kubernetes operator that automates the entire Ceph lifecycle: deployment, scaling, healing, and upgrades â€” all using native Kubernetes resources.  

In this guide, Iâ€™ll walk you through deploying **Rook + Ceph** from scratch and show you how to verify and manage it like a pro.  

---

## ğŸ§  What Youâ€™ll Learn
By the end of this guide, youâ€™ll know how to:
- Deploy the **Rook Operator** and **Ceph Cluster**
- Verify Ceph cluster health from inside Kubernetes  
- Choose and configure **one** storage type: RBD, CephFS, or Object Store  
- Apply production-grade best practices  

---

## âš™ï¸ Prerequisites
Youâ€™ll need:
- A running **Kubernetes cluster** (3+ nodes recommended)  
- `kubectl` access  
- Raw disks or block devices attached to your nodes (for OSDs)  
- Basic knowledge of YAML and Kubernetes resources  

> ğŸ’¡ **Pro tip:** Use Minikube or KIND for learning, and real multi-node clusters with physical disks for production.

---

## ğŸ—ï¸ Step 1: Install Rook Operator and CRDs

Rook extends Kubernetes with **Custom Resource Definitions (CRDs)** representing Ceph components like clusters, pools, filesystems, and gateways.  

Weâ€™ll install those first, then deploy the Rook Operator.

### ğŸ§© 1. Apply CRDs
```bash
kubectl create -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/crds.yaml
```
Registers Ceph-related APIs with Kubernetes.

### ğŸ§© 2. Apply Common Resources
```bash
kubectl create -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/common.yaml
```
Creates the `rook-ceph` namespace and RBAC roles required by the operator.

### ğŸ§  3. Deploy the Operator
```bash
kubectl create -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/operator.yaml
```

Check the pod:
```bash
kubectl -n rook-ceph get pods
```

```
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-operator-6b5d6bb79f-h8x6j   1/1     Running   0          2m
```

The operator now watches for Ceph CRDs and orchestrates deployment.

---

## ğŸ§± Step 2: Create the Ceph Cluster

The `CephCluster` resource defines how Rook should build and configure Ceph â€” monitors, managers, and OSDs.

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    modules:
      - name: pg_autoscaler
        enabled: true
  dashboard:
    enabled: true
  storage:
    useAllNodes: false
```

> âš ï¸ **Important:** Avoid `useAllDevices: true` in production â€” it will claim all disks.

Apply:
```bash
kubectl apply -f ceph-cluster.yaml
```

Check pods:
```bash
kubectl -n rook-ceph get pods
```

```
NAME                                 READY   STATUS    RESTARTS   AGE
rook-ceph-mon-a-6f6b5d79f7-pt7qx     1/1     Running   0          3m
rook-ceph-mgr-a-5d79bcbf7c-8b6dh     1/1     Running   0          2m
rook-ceph-osd-0-7cc94b8b9c-hjs2t     1/1     Running   0          1m
rook-ceph-osd-1-7cc94b8b9c-mtlxm     1/1     Running   0          1m
```

Your Ceph Cluster is now running â€” letâ€™s verify it.

---

## ğŸ§° Step 3: Deploy the Rook Toolbox (and Check Cluster Health)

The **Rook Toolbox** is an administrative pod that includes the `ceph` CLI.

Deploy:
```bash
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/toolbox.yaml
```

When itâ€™s ready:
```bash
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
```

Check cluster health:
```bash
ceph status
```

```
cluster:
  id: a1b2c3d4
  health: HEALTH_OK
  services:
    mon: 3 daemons, quorum a,b,c
    mgr: active: a
    osd: 3 osds: 3 up, 3 in
```

âœ… **Your Ceph cluster is healthy and operational!**

---

## ğŸ§­ Step 4: Choose Your Storage Type

Once healthy, choose **one** storage interface to deploy (per cluster).

| Storage Type | Description | Ideal Use Case |
|--------------|--------------|----------------|
| ğŸ§Š **RBD (Block Storage)** | Dynamic PVCs backed by RBD volumes | Databases, StatefulSets |
| ğŸ“ **CephFS (Filesystem)** | Shared POSIX-compliant filesystem | Shared app data, Prometheus |
| â˜ï¸ **Object Store (RGW)** | S3-compatible object storage | Backups, S3 API apps |

> âš ï¸ **Note:** Deploy only one storage type per cluster based on your application needs.

---

## ğŸ§© Step 5: (Optional) Deploy Multiple Ceph Clusters

For separate environments (e.g., production and staging), apply a second cluster manifest:

```bash
kubectl apply -f cluster-second.yaml
```

Guidelines:
- Different namespace (e.g., `rook-ceph-test`)  
- Unique cluster name and FSID  
- Separate node and disk sets  

---

## ğŸ§­ Rook Ceph Architecture Overview  

Hereâ€™s how **Rook** and **Ceph** integrate within a Kubernetes cluster ğŸ‘‡  

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Kubernetes Cluster                       â”‚
â”‚                                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                   Rook Operator                    â”‚   â”‚
â”‚   â”‚   (Manages Ceph CRDs and orchestrates daemons)     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â”‚           Ceph Cluster         â”‚               â”‚
â”‚           â”‚     (MON / MGR / OSD Pods)     â”‚               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                           â”‚                                â”‚
â”‚               Choose One Storage Interface                 â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚       â”‚                  â”‚                   â”‚             â”‚
â”‚   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”          â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”        â”‚
â”‚   â”‚  RBD  â”‚          â”‚ CephFS â”‚          â”‚  RGW   â”‚        â”‚
â”‚   â”‚ Block â”‚          â”‚  File  â”‚          â”‚ Object â”‚        â”‚
â”‚   â”‚ Store â”‚          â”‚ System â”‚          â”‚  Store â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ§© **In short:**  
- **Rook Operator** manages Ceph lifecycle via CRDs.  
- **Ceph Cluster** runs daemons (MON, MGR, OSD).  
- You can expose one interface â€” **RBD**, **CephFS**, or **RGW** â€” depending on your needs.  

---

## âš¡ Performance & Best Practices

- **Separate roles:** Dedicate nodes to OSDs via taints/tolerations.  
- **Avoid `useAllDevices:true`** in production.  
- **Enable monitoring:** Expose Prometheus metrics to Grafana.  
- **Replication:** 3Ã— replicas for critical data, erasure coding for bulk data.  
- **Routine checks:** Run `ceph health detail` regularly.  

---

## ğŸ¯ Wrapping Up

Youâ€™ve built a **self-healing, scalable storage system** inside Kubernetes â€” powered by Rook and Ceph.  

Rook simplifies Ceph operations; Ceph delivers enterprise-grade resilience and performance.  
Together they form a foundation for stateful, cloud-native workloads.  

Next steps:
- Enable the **Ceph Dashboard**  
- Connect **Prometheus + Grafana**  
- Explore **CephFS or RGW**  
- Learn backup and recovery workflows  

---

## âœ¨ Final Thoughts

Kubernetes isnâ€™t just for stateless apps anymore â€” with Rook Ceph, it becomes a true **data platform**.  

If you found this helpful, follow me for future posts in my **Rook Ceph Deep Dive Series**, where Iâ€™ll cover:  
- ğŸ§° Ceph Dashboard setup and metrics  
- ğŸ’¾ CephFS and RGW performance tuning  
- ğŸ›¡ï¸ Recovery from OSD or MON failures  

---

### ğŸ§‘â€ğŸ’» About the Author  

Hi there! Iâ€™m **Dhinakaran J**, a **Site Reliability Engineer at the National Payments Corporation of India (NPCI)** â€” where I help ensure the **availability, scalability, and reliability of Indiaâ€™s critical payment infrastructure**.  

I work extensively with **Kubernetes**, **Rook Ceph**, **Argo CD**, **Prometheus**, **Docker**, **RKE**, **Jenkins**, **MinIO**, and **Grafana**, building automated and observable cloud-native systems that keep payment services running smoothly at scale.  

My passion lies in **infrastructure reliability**, **monitoring**, and **distributed storage systems** â€” especially leveraging **Rook Ceph** to deliver highly resilient and transparent platforms in Kubernetes.  

When Iâ€™m not optimizing clusters or debugging production workloads, I enjoy exploring open-source technologies, mentoring engineers, and writing hands-on DevOps guides to make complex systems simple for everyone.  

> ğŸ’¬ Follow me on Medium for practical guides on **Kubernetes**, **Ceph**, **Argo CD**, and **Observability** â€” drawn from real-world SRE experience.  

---

**Short footer bio:**  
> *Dhinakaran J is a Site Reliability Engineer at NPCI specializing in Kubernetes, Ceph, Argo CD, Prometheus, and cloud-native reliability.*  

---

### ğŸ·ï¸ Suggested Tags for Medium  
`#Kubernetes`  `#DevOps`  `#Ceph`  `#Rook`  `#ArgoCD`  `#Prometheus`  `#CloudNative`  `#SRE`  `#Storage`

---

ğŸ“¸ **Suggested Image Placeholders for Medium**
1. *Header Image:* Kubernetes + Ceph architecture visual.  
2. *Step 1 Screenshot:* `kubectl get pods -n rook-ceph` showing operator running.  
3. *Step 3 Screenshot:* Ceph dashboard or `ceph status` output.  
4. *Architecture Diagram:* Use the above ASCII version converted to a PNG for a polished look.  
